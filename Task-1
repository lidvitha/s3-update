package com.example.sparkmerge;

import org.apache.spark.sql.*;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.Column;

import java.util.Arrays;
import java.util.List;

import static org.apache.spark.sql.functions.*;

public class MergeKeysSortKeysTest {

    public static void main(String[] args) {
        // 1) Create SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("MergeKeysSortKeysTest")
                .master("local[*]")   // For local testing
                .getOrCreate();

        try {
            // 2) Define paths for your previous day (refined) & current day (trusted)
            //    Adjust these to match your actual S3 or local paths
            String previousDayPath = "s3a://my-refined-bucket/090/eod/accounts/year=2023/month=02/day=23/";
            String currentDayPath  = "s3a://my-trusted-bucket/090/event/loanAccounts/year=2023/month=02/day=24/";

            // 3) Read both datasets
            System.out.println("Reading previous day from: " + previousDayPath);
            Dataset<Row> previousDayDS = spark.read()
                    .option("mergeSchema", "true") // If your files have slightly different schemas
                    .parquet(previousDayPath);

            System.out.println("Reading current day from: " + currentDayPath);
            Dataset<Row> currentDayDS = spark.read()
                    .option("mergeSchema", "true")
                    .parquet(currentDayPath);

            // 4) Merge keys & sort keys (Hard-coded example)
            //    In practice, you'd parse these from your JSON request.
            List<String> mergeKeys = Arrays.asList("accountNumber", "enterpriseProductClassificationCode");
            List<String> sortKeys = Arrays.asList("executionTimestamp");

            // 5) Convert mergeKeys to Spark Columns for partitioning
            Column[] partitionCols = mergeKeys.stream()
                    .map(functions::col)
                    .toArray(Column[]::new);

            // 6) Convert sortKeys to Spark Columns (descending example)
            List<Column> sortCols = sortKeys.stream()
                    .map(k -> functions.col(k).desc())
                    .toList();

            // 7) Union the datasets by name (allow missing columns), then repartition
            Dataset<Row> unioned = previousDayDS
                    .unionByName(currentDayDS, true)  // allowMissingColumns = true
                    .repartition(4, partitionCols);    // example partition count = 4

            // 8) Show the unioned data BEFORE pick-first or custom transform
            long unionedCount = unioned.count();
            System.out.println("=== After union & partitioning, row count: " + unionedCount + " ===");
            unioned.show(20, false);

            // 9) Pick-first logic using collect_list + sort_array
            //    - We group by the merge keys
            //    - We collect each row into a struct that includes executionTimestamp plus all columns
            //    - We sort the array in descending order, then pick the first item (the newest row)
            unioned = unioned.groupBy(partitionCols)
                    .agg(
                            sort_array(
                                    collect_list(
                                            struct(col("executionTimestamp"), col("*"))
                                    ),
                                    false // false => descending
                            ).getItem(0).alias("row")
                    )
                    .select("row.*");

            long finalCount = unioned.count();
            System.out.println("=== After pick-first logic, row count: " + finalCount + " ===");
            unioned.show(20, false);

            // 10) Write final merged data to a new path or console for verification
            String finalPath = "s3a://my-refined-bucket/090/eod/accounts/year=2023/month=02/day=24-merged/";
            System.out.println("Writing final merged data to: " + finalPath);
            unioned.write().mode(SaveMode.Overwrite).parquet(finalPath);

        } catch (Exception e) {
            System.out.println("Error merging datasets: " + e.getMessage());
            e.printStackTrace();
        } finally {
            spark.stop();
        }
    }
}



package com.example.sparkmerge;

import org.apache.spark.sql.*;
import org.apache.spark.sql.Column;
import static org.apache.spark.sql.functions.*;

import java.util.List;
import java.util.Arrays;

public class PickFirstWindowExample {

    public static void main(String[] args) {
        // For Windows: set hadoop.home.dir if needed (winutils.exe in C:/hadoop/bin)
        System.setProperty("hadoop.home.dir", "C:/hadoop");

        SparkSession spark = SparkSession.builder()
                .appName("PickFirstWindowExample")
                .master("local[*]")
                .getOrCreate();

        try {
            // 1) Define local file paths for previous day & current day
            String previousDayPath = "file:///C:/Users/008312Y/pir-test-data/refined-bucket/accounts/day=23";
            String currentDayPath  = "file:///C:/Users/008312Y/pir-test-data/trusted-bucket/accounts/day=24";

            System.out.println("Reading previous day from: " + previousDayPath);
            Dataset<Row> previousDayDS = spark.read()
                    .option("mergeSchema", "true")
                    .parquet(previousDayPath);

            System.out.println("Reading current day from: " + currentDayPath);
            Dataset<Row> currentDayDS = spark.read()
                    .option("mergeSchema", "true")
                    .parquet(currentDayPath);

            // 2) Union by name, allowing missing columns
            Dataset<Row> unioned = previousDayDS.unionByName(currentDayDS, true);

            // 3) Same logic snippet: withColumn(... coalesce(...)), then repartition
            unioned = unioned
                    .withColumn(
                        "updateTimestamp",
                        coalesce(col("accountLastUpdatedTimestamp"), col("accountOpenTimestamp"))
                    )
                    .repartition(
                        4, // example partition count
                        col("accountNumber"),
                        col("enterpriseProductClassificationCode")
                    );

            // 4) "Pick-first" logic using PickFirstFromGroup (no window function):
            //    - Unioned dataset is transformed with a custom aggregator
            //    - We group by (accountNumber, enterpriseProductClassificationCode)
            //    - We pick the row with the largest publishedTimestamp
            unioned = unioned.transform(
                new PickFirstFromGroup(
                    // merge keys
                    Arrays.asList("accountNumber", "enterpriseProductClassificationCode"),
                    // sort keys (descending on publishedTimestamp)
                    Arrays.asList(col("publishedTimestamp").desc())
                )
            );

            // 5) Show the final results
            long finalCount = unioned.count();
            System.out.println("=== Final pick-first dataset row count: " + finalCount + " ===");
            unioned.show(50, false);

            // 6) Write to local output path
            String finalPath = "file:///C:/Users/008312Y/pir-test-data/output/final-pick-first-window";
            System.out.println("Writing final data to: " + finalPath);
            unioned.write().mode(SaveMode.Overwrite).parquet(finalPath);

        } catch (Exception e) {
            System.out.println("Error: " + e.getMessage());
            e.printStackTrace();
        } finally {
            spark.stop();
        }
    }
}

