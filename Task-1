package com.example.sparkmerge;

import org.apache.spark.sql.*;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.Column;

import java.util.Arrays;
import java.util.List;

import static org.apache.spark.sql.functions.*;

public class MergeKeysSortKeysTest {

    public static void main(String[] args) {
        // 1) Create SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("MergeKeysSortKeysTest")
                .master("local[*]")   // For local testing
                .getOrCreate();

        try {
            // 2) Define paths for your previous day (refined) & current day (trusted)
            //    Adjust these to match your actual S3 or local paths
            String previousDayPath = "s3a://my-refined-bucket/090/eod/accounts/year=2023/month=02/day=23/";
            String currentDayPath  = "s3a://my-trusted-bucket/090/event/loanAccounts/year=2023/month=02/day=24/";

            // 3) Read both datasets
            System.out.println("Reading previous day from: " + previousDayPath);
            Dataset<Row> previousDayDS = spark.read()
                    .option("mergeSchema", "true") // If your files have slightly different schemas
                    .parquet(previousDayPath);

            System.out.println("Reading current day from: " + currentDayPath);
            Dataset<Row> currentDayDS = spark.read()
                    .option("mergeSchema", "true")
                    .parquet(currentDayPath);

            // 4) Merge keys & sort keys (Hard-coded example)
            //    In practice, you'd parse these from your JSON request.
            List<String> mergeKeys = Arrays.asList("accountNumber", "enterpriseProductClassificationCode");
            List<String> sortKeys = Arrays.asList("executionTimestamp");

            // 5) Convert mergeKeys to Spark Columns for partitioning
            Column[] partitionCols = mergeKeys.stream()
                    .map(functions::col)
                    .toArray(Column[]::new);

            // 6) Convert sortKeys to Spark Columns (descending example)
            List<Column> sortCols = sortKeys.stream()
                    .map(k -> functions.col(k).desc())
                    .toList();

            // 7) Union the datasets by name (allow missing columns), then repartition
            Dataset<Row> unioned = previousDayDS
                    .unionByName(currentDayDS, true)  // allowMissingColumns = true
                    .repartition(4, partitionCols);    // example partition count = 4

            // 8) Show the unioned data BEFORE pick-first or custom transform
            long unionedCount = unioned.count();
            System.out.println("=== After union & partitioning, row count: " + unionedCount + " ===");
            unioned.show(20, false);

            // 9) Pick-first logic using collect_list + sort_array
            //    - We group by the merge keys
            //    - We collect each row into a struct that includes executionTimestamp plus all columns
            //    - We sort the array in descending order, then pick the first item (the newest row)
            unioned = unioned.groupBy(partitionCols)
                    .agg(
                            sort_array(
                                    collect_list(
                                            struct(col("executionTimestamp"), col("*"))
                                    ),
                                    false // false => descending
                            ).getItem(0).alias("row")
                    )
                    .select("row.*");

            long finalCount = unioned.count();
            System.out.println("=== After pick-first logic, row count: " + finalCount + " ===");
            unioned.show(20, false);

            // 10) Write final merged data to a new path or console for verification
            String finalPath = "s3a://my-refined-bucket/090/eod/accounts/year=2023/month=02/day=24-merged/";
            System.out.println("Writing final merged data to: " + finalPath);
            unioned.write().mode(SaveMode.Overwrite).parquet(finalPath);

        } catch (Exception e) {
            System.out.println("Error merging datasets: " + e.getMessage());
            e.printStackTrace();
        } finally {
            spark.stop();
        }
    }
}
