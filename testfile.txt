package com.example;

import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.PutObjectRequest;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.kafka.connect.header.Header;
import org.apache.kafka.connect.sink.SinkRecord;
import org.apache.kafka.connect.sink.SinkTask;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetFileWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import org.apache.parquet.hadoop.util.HadoopOutputFile;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.text.SimpleDateFormat;
import java.util.*;

public class S3SinkTask extends SinkTask {
    private AmazonS3 s3Client;
    private String bucketName;
    private Map<String, List<SinkRecord>> topicBuffers;
    private Map<String, Long> topicLastFlushTimes;
    private int batchSize;
    private long batchTimeMs;
    private int eventCounter = 0;
    private static final Logger log = LoggerFactory.getLogger(S3SinkTask.class);

    private static final Schema SCHEMA = new Schema.Parser().parse(
            "{\n" +
                    "  \"type\": \"record\",\n" +
                    "  \"name\": \"NonDecomposeFormat\",\n" +
                    "  \"namespace\": \"com.example\",\n" +
                    "  \"fields\": [\n" +
                    "    {\"name\": \"eventId\", \"type\": [\"null\", \"string\"], \"default\": null},\n" +
                    "    {\"name\": \"partition\", \"type\": [\"null\", \"string\"], \"default\": null},\n" +
                    "    {\"name\": \"payload\", \"type\": [\"null\", \"string\"], \"default\": null},\n" +
                    "    {\"name\": \"updDate\", \"type\": [\"null\", \"string\"], \"default\": null},\n" +
                    "    {\"name\": \"publishTimestampStr\", \"type\": [\"null\", \"string\"], \"default\": null},\n" +
                    "    {\"name\": \"topic\", \"type\": [\"null\", \"string\"], \"default\": null},\n" +
                    "    {\"name\": \"eventName\", \"type\": [\"null\", \"string\"], \"default\": null},\n" +
                    "    {\"name\": \"eventVersionNumber\", \"type\": [\"null\", \"string\"], \"default\": null},\n" +
                    "    {\"name\": \"executionTimestampStr\", \"type\": [\"null\", \"string\"], \"default\": null}\n" +
                    "  ]\n" +
                    "}"
    );

    @Override
    public void start(Map<String, String> props) {
        String accessKeyId = props.get(S3SinkConfig.AWS_ACCESS_KEY_ID);
        String secretAccessKey = props.get(S3SinkConfig.AWS_SECRET_ACCESS_KEY);
        bucketName = props.get(S3SinkConfig.S3_BUCKET_NAME);

        BasicAWSCredentials awsCreds = new BasicAWSCredentials(accessKeyId, secretAccessKey);
        s3Client = AmazonS3ClientBuilder.standard()
                .withRegion(Regions.fromName(props.get(S3SinkConfig.S3_REGION)))
                .withCredentials(new AWSStaticCredentialsProvider(awsCreds))
                .build();

        topicBuffers = new HashMap<>();
        topicLastFlushTimes = new HashMap<>();

        batchSize = Integer.parseInt(props.get(S3SinkConfig.S3_BATCH_SIZE));
        batchTimeMs = Long.parseLong(props.get(S3SinkConfig.S3_BATCH_TIME_MS));
    }

    @Override
    public void put(Collection<SinkRecord> records) {
        for (SinkRecord record : records) {
            String topic = record.topic();
            topicBuffers.computeIfAbsent(topic, k -> new ArrayList<>()).add(record);

            if (topicBuffers.get(topic).size() >= batchSize || (System.currentTimeMillis() - topicLastFlushTimes.getOrDefault(topic, 0L)) >= batchTimeMs) {
                flushRecords(topic);
            }
        }
    }

    private void flushRecords(String topic) {
        if (!topicBuffers.get(topic).isEmpty()) {
            try {
                String key = generateFileKey();
                Path tempFile = Files.createTempFile("parquet", ".parquet");

                try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(
                                HadoopOutputFile.fromPath(new org.apache.hadoop.fs.Path(tempFile.toString()), new org.apache.hadoop.conf.Configuration()))
                        .withSchema(SCHEMA)
                        .withWriteMode(ParquetFileWriter.Mode.OVERWRITE)
                        .withCompressionCodec(CompressionCodecName.SNAPPY)
                        .build()) {

                    for (SinkRecord record : topicBuffers.get(topic)) {
                        GenericRecord avroRecord = new GenericData.Record(SCHEMA);
                        avroRecord.put("eventId", record.key());
                        avroRecord.put("partition", record.kafkaPartition());
                        avroRecord.put("payload", new String((byte[]) record.value(), StandardCharsets.UTF_8));
                        avroRecord.put("updDate", new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss'Z'").format(new Date()));
                        avroRecord.put("publishTimestampStr", record.timestamp());
                        avroRecord.put("topic", record.topic());
                        avroRecord.put("eventName", null); // Set as per your logic
                        avroRecord.put("eventVersionNumber", null); // Set as per your logic
                        avroRecord.put("executionTimestampStr", null); // Set as per your logic

                        writer.write(avroRecord);
                    }
                }

                s3Client.putObject(new PutObjectRequest(bucketName, key, tempFile.toFile()));
                topicBuffers.get(topic).clear();
                topicLastFlushTimes.put(topic, System.currentTimeMillis());
                Files.delete(tempFile);
            } catch (IOException e) {
                log.error("Error flushing records", e);
            }
        }
    }

    private String generateFileKey() {
        String timestamp = new SimpleDateFormat("yyyyMMddHHmmss").format(new Date());
        return String.format("event%d-%s.parquet", eventCounter++, timestamp);
    }

    @Override
    public void stop() {
        for (String topic : topicBuffers.keySet()) {
            if (!topicBuffers.get(topic).isEmpty()) {
                flushRecords(topic);
            }
        }
    }

    @Override
    public String version() {
        return "1.0";
    }
}]


.checkbox-container {
    display: flex;
    justify-content: center;
    align-items: center;
    background: linear-gradient(to right, #6a11cb, #2575fc); /* Unique gradient */
    padding: 15px 20px;
    border-radius: 12px;
    box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.2);
    max-width: 600px;
    margin: 20px auto;
    gap: 15px;
}

.checkbox-label {
    display: flex;
    align-items: center;
    font-size: 18px;
    font-weight: 600;
    color: white;
    cursor: pointer;
    padding: 8px 12px;
    border-radius: 20px;
    background: rgba(255, 255, 255, 0.2);
    transition: all 0.3s ease-in-out;
}

.checkbox-label:hover {
    background: rgba(255, 255, 255, 0.4);
}

.checkbox-input {
    appearance: none;
    width: 20px;
    height: 20px;
    border: 2px solid white;
    border-radius: 50%;
    margin-right: 8px;
    position: relative;
    cursor: pointer;
    transition: all 0.3s ease-in-out;
}

.checkbox-input:checked {
    background-color: white;
}

.checkbox-input:checked::before {
    content: 'âœ”';
    font-size: 14px;
    font-weight: bold;
    color: #2575fc;
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
}

<div className="checkbox-container">
    <label className="checkbox-label">
        <input
            type="checkbox"
            className="checkbox-input"
            onChange={() => handleChange("Active")}
        />
        Active
    </label>

    <label className="checkbox-label">
        <input
            type="checkbox"
            className="checkbox-input"
            onChange={() => handleChange("Inactive")}
        />
        Inactive
    </label>

    <label className="checkbox-label">
        <input
            type="checkbox"
            className="checkbox-input"
            onChange={() => handleChange("Sale")}
        />
        Sale
    </label>
</div>



__________________
import React from "react";
import { Navbar, Nav, Button, Container } from "react-bootstrap";

const CustomNavbar = ({ handleShowTopTen, handleExportHistory }) => {
  return (
    <Navbar bg="dark" variant="dark" expand="lg">
      <Container>
        <Navbar.Brand href="#">Fulfillment Dashboard</Navbar.Brand>
        <Navbar.Toggle aria-controls="basic-navbar-nav" />
        <Navbar.Collapse id="basic-navbar-nav">
          <Nav className="ms-auto">
            <Button variant="outline-light" className="me-2" onClick={handleShowTopTen}>
              Top 10 Fulfillments
            </Button>
            <Button variant="outline-light" onClick={handleExportHistory}>
              Export Fulfillment History
            </Button>
          </Nav>
        </Navbar.Collapse>
      </Container>
    </Navbar>
  );
};

export default CustomNavbar;


