package com.example.sparkcompare;

import org.apache.spark.sql.*;
import org.apache.spark.sql.types.StructType;
import static org.apache.spark.sql.functions.*;

public class FullOuterJoinCompare {

    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("FullOuterJoinCompare")
                .master("local[*]")
                .getOrCreate();

        try {
            // Define local file paths for the two Parquet files.
            String parquetPathA = "file:///C:/Users/008312Y/mydata/parquetA";
            String parquetPathB = "file:///C:/Users/008312Y/mydata/parquetB";

            // Read both Parquet files.
            Dataset<Row> dfA = spark.read().parquet(parquetPathA);
            Dataset<Row> dfB = spark.read().parquet(parquetPathB);

            // Print schemas for verification
            System.out.println("Schema of dfA:");
            dfA.printSchema();
            System.out.println("Schema of dfB:");
            dfB.printSchema();

            // Ensure both DataFrames have the same column order if possible.
            // This is important for a row-wise comparison.
            dfB = dfB.select(dfA.columns());

            // Create a join condition that compares each column for equality.
            // For simplicity, we assume the DataFrames have the same schema.
            String[] columns = dfA.columns();
            Column joinCondition = lit(true); // start with a condition that's always true
            for (String colName : columns) {
                joinCondition = joinCondition.and(dfA.col(colName).equalTo(dfB.col(colName)));
            }

            // Perform a full outer join on the condition.
            Dataset<Row> joined = dfA.join(dfB, joinCondition, "full_outer");

            // Now, any row where one side is missing will have null in the corresponding columns.
            // Also, if there are mismatches, the equality condition will fail.
            // Let's filter out rows where the values from dfA and dfB are not equal.
            // One approach is to check for a null on one side of a key column.
            // Here we assume there's at least one column (e.g. "accountNumber") to test:
            Dataset<Row> differences = joined.filter(
                dfA.col("accountNumber").isNull().or(dfB.col("accountNumber").isNull())
            );

            long diffCount = differences.count();
            if (diffCount == 0) {
                System.out.println("Both Parquet files have exactly the same rows.");
            } else {
                System.out.println("Differences found! Rows that do not match:");
                differences.show(false);
            }
            
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            spark.stop();
        }
    }
}


package com.example.sparkcompare;

import org.apache.spark.sql.*;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.expressions.WindowSpec;
import org.apache.spark.sql.types.StructType;

import static org.apache.spark.sql.functions.*;

public class RowByRowCompareSorted {

    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("RowByRowCompareSorted")
                .master("local[*]")
                .getOrCreate();

        try {
            // Define paths for the two Parquet files (already sorted)
            String parquetPathA = "file:///C:/Users/008312Y/mydata/parquetA";
            String parquetPathB = "file:///C:/Users/008312Y/mydata/parquetB";

            // Read both Parquet files
            Dataset<Row> dfA = spark.read().parquet(parquetPathA);
            Dataset<Row> dfB = spark.read().parquet(parquetPathB);

            // Ensure both DataFrames have the same columns in the same order.
            dfB = dfB.select(dfA.columns());

            // Add a row index to each DataFrame using a window with a constant ordering.
            // Because the data is already sorted, ordering by a constant works fine.
            WindowSpec w = Window.orderBy(lit(1));
            Dataset<Row> dfAIndexed = dfA.withColumn("row_index", row_number().over(w));
            Dataset<Row> dfBIndexed = dfB.withColumn("row_index", row_number().over(w));

            // Join on the row_index so that row 1 of dfA is compared with row 1 of dfB, etc.
            Dataset<Row> joined = dfAIndexed.alias("A")
                    .join(dfBIndexed.alias("B"), col("A.row_index").equalTo(col("B.row_index")), "full_outer")
                    .orderBy("A.row_index");

            // Build a condition that flags any row where any column value differs.
            // We assume both DataFrames have the same schema (excluding the row_index column).
            String[] commonColumns = dfA.columns();
            Column diffCondition = lit(false);
            for (String c : commonColumns) {
                diffCondition = diffCondition.or(col("A." + c).notEqual(col("B." + c)));
            }

            // Filter the joined DataFrame for rows where differences exist.
            Dataset<Row> mismatches = joined.filter(diffCondition);

            // Print the row counts and mismatches
            long countA = dfAIndexed.count();
            long countB = dfBIndexed.count();
            System.out.println("Row count in dfA: " + countA);
            System.out.println("Row count in dfB: " + countB);

            long diffCount = mismatches.count();
            if (diffCount == 0) {
                System.out.println("Both Parquet files have identical rows (row-by-row).");
            } else {
                System.out.println("Differences found in " + diffCount + " rows:");
                mismatches.show(false);
            }

        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            spark.stop();
        }
    }
}

